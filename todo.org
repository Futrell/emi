
#+TITLE: Profile NLP Algorithms
#+AUTHOR: Chad Kringen
#+DATE:<2017-06-27 Tue>

* Overview
Investigate how to profile NLP algorithms in python, look for ways to improve performance.
 
* Objectives
** TODO slurm scripts
** DONE working examples of cython
   CLOSED: [2017-07-03 Mon 10:12]
** DONE working examples of numba
   CLOSED: [2017-07-06 Thu 12:26]
** DONE performance/profiling framework
   CLOSED: [2017-07-06 Thu 01:25]
Three profiling strategies: 

1. cProfile : comes in the stdlib, used in conjunction with pstats, good overall
#+BEGIN_SRC 
python -m cProfile script.py
#+END_SRC

2. line_profiler : third party, line-by-line runtime information
#+BEGIN_SRC 
kernprof -l -m <script.py> -o prof.out
python -m line_profiler prof.out
#+END_SRC

3. memory_profiler : third party, line-by-line information on memory consumption
#+BEGIN_SRC 
python -m memory_profiler <script.py>
#+END_SRC

Each needs "@profile" above the functions intended to be profiled in the source code.  
To this end, we have decided to modify the code itself at runtime, to add the profile
decorator to the appropriate list of functions.  So, the performance testing scripts
import the source code as usual, but at this point, we employ the ast library (in the
stdlib) to modify the syntax tree of the source code, to add the decorator, then use
write performance tests around these functions.

** DONE code reorganization
   CLOSED: [2017-07-06 Thu 17:45]
Need setup.py to register module paths, treat overall project as a module.
Further, to use overall, after cloning, need to run:

#+BEGIN_SRC 
python setup.py build
python setup.py install
#+END_SRC

** DONE tests for existing code
   CLOSED: [2017-07-06 Thu 01:25]
*** cython
*** numba
*** bits of count_skipgrams.py
*** overall methodology

** TODO refactor profiling module 
   Base off of google benchmark, which I used in a previous C++ project.
   The idea is that make every test a standalone function, e.g. a static/global func, 
   and then decorate those with a profile class that gives us an iterator, 
   maybe a setup/tear-down method.

   We also need to refactor how we call the runProf.sh script.  I think we only need one flag
   for the type of profiler, and an output file argument.

   _sample google benchmark output_

    | Benchmark                | Time    | CPU      | Iterations |                UserCounters |
    |--------------------------+---------+----------+------------+-----------------------------|
    | BM_UserCounter/threads:8 | 2248 ns | 10277 ns |      68808 |  Bar=16 Bat=40 Baz=24 Foo=8 |
    | BM_UserCounter/threads:1 | 9797 ns | 9788 ns  |      71523 | Bar=2 Bat=5 Baz=3 Foo=1024m |
    | BM_CalculatePiRange/1    | 16 ns   | 16 ns    |   45704255 |                           0 |
    | BM_CalculatePiRange/8    | 73 ns   | 73 ns    |    9520927 |                     3.28374 |
    | BM_CalculatePiRange/64   | 609 ns  | 609 ns   |    1140647 |                     3.15746 |
    |                          |         |          |            |                             |
                  
*** TODO profile class for each profiler
    1. [X] cProfile
    2. [ ] MemoryProfiler
    3. [ ] LineProfiler
    4. [X] time

*** TODO ranges macro
    I think this should work just like a decorator that builds a chunk of data according to some given parameter
    and then calls the to-be-profiled function on it repeatedly for larger and larger data

*** DONE print stats data as csv
    CLOSED: [2017-07-12 Wed 19:51]
    We ended up just doing main calls from the cProfile getstats( ) function.  This omits subcalls,
    as well as obfuscates caller v. callee.  To that end, we also just wrap the pstats module's print function

*** DONE print each benchmark to its own file, or delineate in some way
    CLOSED: [2017-07-24 Mon 14:56]
*** DONE get some results
    CLOSED: [2017-07-14 Fri 10:32]
*** DONE clean up runProf.sh
    CLOSED: [2017-07-19 Wed 18:32]
    parse command line arguments in python, just use script as top-level, create/move profile reports
*** DONE build benchmarks class
    CLOSED: [2017-07-19 Wed 18:34]
    setUp and tearDown methods, keeps track of functions to profile, gets import-hooked
*** DONE import hook 
    CLOSED: [2017-07-19 Wed 18:34]
    Ultimately decided to avoid AST for now, and create our own __import__ function instead,
    taking care of source code augmentation using get/setattr
*** DONE redesign current benchmark class
    CLOSED: [2017-07-25 Tue 19:55]
    clean up instance-based stuff, possibly move bare functions to
    separate file so as not to clutter the import space of the class itself
*** DONE figure out when import hook fails
    CLOSED: [2017-07-23 Sun 21:27]
    best lead: sub-sub-packages?
    answer: not actually sure.  just going to avoid overriding a builtin,
            and use basic introspection

** DONE concurrency
   CLOSED: [2017-08-03 Thu 13:24]
   Try factoring out the data processing pipeline (chunking, tokenizing, skipgramming, counting) using concurrent tools
*** DONE async
    CLOSED: [2017-07-14 Fri 10:35]
    Use the asyncio library for lots of small tasks that don't take too long to complete

*** DONE processes
    CLOSED: [2017-08-03 Thu 13:24]
    The major tool here is the multiprocessing (multithreading) module from the stdlib, I think.  There must be 
    some interesting interaction between multiprocessing and the slurm scheduler, right?
*** DONE concurrent.futures
    CLOSED: [2017-08-03 Thu 13:24]
    see what threads can do for us
** DONE memory map files to feed to functions
   CLOSED: [2017-07-14 Fri 10:35]
   Need to just benchmark this.  Time access for a large file via:
   1. readlines/yield statement
   2. binary read
   3. mmapp'ed binary read
** DONE runProf.sh
   CLOSED: [2017-08-03 Thu 13:24]
*** DONE collect just the "-f" option, pass everything else onto python
    CLOSED: [2017-08-03 Thu 13:24]
*** DONE put getopts in a function? weirdly doesn't work
    CLOSED: [2017-08-03 Thu 13:24]
** TODO fixtures for each style of skipgramming   
   Run all the below from runProf.sh.
   1. [ ] async
   2. [ ] threads
   3. [ ] processes
   4. [ ] generators
   5. [ ] data sources

** TODO cython versions of the skipgramming pipelines
** TODO theory for investigating performance
   We have a number of tools to investigate performance,
  
   1. line_profiler
   2. memory_profiler
   3. cProfile
   4. disassembler
   5. traceback
   6. timer

   but not a lot of theory to really use the tools.  For instance, how do we 
   read the bytecode output, what do we look for?  Similarly, how do we know
   what's going on in the actual running of the code?

   Essentially all we have is a relativistic approach:

       *compare two pieces of code side by side*


* Directives
2017-05-10

Using output/vp_observations.csv, which includes data about verb particle positioning in transitive verbs and information
about the direct object, I find the following. When a verb and particle have pmi, then they are more likely to be adjacent.
Furthermore, when the direct object is long AND the verb and particle have high pmi, then the particle is even more likely
to be close (an interaction exists). This is the predict distance-pmi interaction in ordering preferences.



2017-05-08

data/vps.txt comes from Stefan Gries's book.
data/verbs.regex is the verbs from those.
code/filter_v.sh filters for those verbs.

To get verb-particle counts, do
python2 query.py '(VB|VBD|VBG|VBN|VBP|VBZ) >prt _' -m 0 -d '/om/user/futrell/en00aa.data/*.db' | python2 querypairs.py | sed "s/^.*\g//g" | python2 lemmatize_verbs.py | sh filter_v.sh > prtless_verbs.txt


We need the counts of how often these verbs appear *without* particles.
To do this,
yse dep_search on the first parsed Common Crawl Parse file.
python2 query.py '(VB|VBD|VBG|VBN|VBP|VBZ) !>prt _' -m 0 -d '/om/user/futrell/en00aa.data/*.db' | python2 querypairs.py | sed "s/^.*\g//g" | python2 lemmatize_verbs.py | sh filter_v.sh > prtless_verbs.txt

Grab all verb-prt->_ things; lemmatize the verbs; filter them to be from Stefan Gries's list of verbs; then save those.



--------------------

OK, I gave you access to the repo with the code for this project.
The main pipeline for getting skipgram counts from the Common Crawl data is in code/countsortmerge.sh.
Once you get an MIT guest account and an OpenMind account (probably tomorrow), you will be able to try running it against the data and we can figure out if it would be possible to speed this up a lot.

Best, R

* Useful Links and Information

-- slurm scheduler
https://slurm.schedmd.com/


-- itertools library
http://code.activestate.com/recipes/305588-simple-example-to-show-off-itertoolstee/
https://stackoverflow.com/questions/6703594/is-the-result-of-itertools-tee-thread-safe-python

https://stackoverflow.com/questions/13628934/itertools-islice-implementation-efficiently-slicing-a-list


-- setting up the project as a moddule in good python fashion
https://pythonhosted.org/an_example_pypi_project/setuptools.html


-- python, general
https://julien.danjou.info/blog/2013/guide-python-static-class-abstract-methods


-- google benchmark
https://github.com/google/benchmark


-- multithreaded python
https://github.com/sampsyo/cluster-workers

https://stackoverflow.com/questions/25904537/how-do-i-send-data-to-a-running-python-thread

https://stackoverflow.com/questions/14508906/sending-messages-between-class-threads-python

http://wla.berkeley.edu/~cs61a/fa11/lectures/
